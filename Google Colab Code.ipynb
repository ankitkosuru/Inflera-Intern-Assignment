{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup & Installs\n",
    "!pip -q install langchain langchain-community langchain-text-splitters \\\n",
    "               faiss-cpu sentence-transformers gpt4all gradio requests numexpr\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 2) Upload your files\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Select the six files from your computer\n",
    "print(\"Uploaded:\", list(uploaded.keys()))\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 3) Ingestion: load → chunk → embed → index\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Ensure all expected files exist in the current working directory\n",
    "EXPECTED_FILES = [\n",
    "    \"amazon_sales_stats.txt\",\n",
    "    \"amazon_echo_dot_reviews.txt\",\n",
    "    \"amazon_echo_dot_manual.txt\",\n",
    "    \"amazon_policy_guidelines.txt\",\n",
    "    \"amazon_echo_dot_specs.txt\",\n",
    "    \"amazon_faq.txt\",\n",
    "]\n",
    "missing = [f for f in EXPECTED_FILES if not os.path.exists(f)]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing files: {missing}. Please upload them in the previous cell.\")\n",
    "\n",
    "# Load documents with source metadata\n",
    "docs = []\n",
    "for path in EXPECTED_FILES:\n",
    "    loader = TextLoader(path, encoding=\"utf-8\")\n",
    "    for d in loader.load():\n",
    "        # Keep filename as \"source\" for UI\n",
    "        d.metadata[\"source\"] = os.path.basename(path)\n",
    "        docs.append(d)\n",
    "\n",
    "# Chunk\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "# Embeddings (local, no API key needed)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Vector store\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "# Convenience retriever\n",
    "def retrieve(query: str, k: int = 3):\n",
    "    \"\"\"Return top-k (doc, score) using FAISS similarity_search_with_score (lower score = closer).\"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    return results\n",
    "\n",
    "print(f\"Loaded {len(docs)} docs | {len(splits)} chunks indexed.\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Step 4: LLM using Hugging Face Flan-T5-Base\n",
    "!pip -q install transformers accelerate\n",
    "\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# Load Flan-T5-base\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Build pipeline\n",
    "hf_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    do_sample=False  # deterministic\n",
    ")\n",
    "\n",
    "# Wrap for LangChain\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "print(\"✅ Flan-T5-base ready as LLM\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# Generic, Domain-Agnostic RAG (no special rules)\n",
    "# - Better chunking\n",
    "# - Hybrid retrieval: Embeddings (FAISS, MMR) + BM25\n",
    "# - Optional Cross-Encoder re-ranking (fallback-safe)\n",
    "# - Strict grounded answer chain with GK fallback\n",
    "# - Utility: retrieve() returns (Document, pseudo_distance) for your UI\n",
    "# ============================================\n",
    "\n",
    "# 0) Installs (Colab safe). Re-run if kernel restarts.\n",
    "!pip -q install langchain langchain-community sentence-transformers rank-bm25 faiss-cpu\n",
    "\n",
    "# 1) Imports\n",
    "import os, glob, math, numpy as np\n",
    "from typing import List, Tuple\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "# Optional cross-encoder re-ranker (falls back automatically if not available)\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    _CROSS_ENCODER = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "except Exception:\n",
    "    _CROSS_ENCODER = None\n",
    "\n",
    "# 2) Load your uploaded files (adjust paths if needed)\n",
    "def _collect_files():\n",
    "    paths = []\n",
    "    # Common Colab paths: /content, and your notebook-mounted /mnt/data\n",
    "    for root in [\"/mnt/data\", \"/content\"]:\n",
    "        if os.path.isdir(root):\n",
    "            paths += glob.glob(os.path.join(root, \"*.txt\"))\n",
    "            paths += glob.glob(os.path.join(root, \"*.md\"))\n",
    "    # De-dup while preserving order\n",
    "    seen, ordered = set(), []\n",
    "    for p in paths:\n",
    "        if p not in seen:\n",
    "            seen.add(p)\n",
    "            ordered.append(p)\n",
    "    return ordered\n",
    "\n",
    "FILE_PATHS = _collect_files()\n",
    "assert FILE_PATHS, \"No .txt/.md files found in /mnt/data or /content. Upload your files first.\"\n",
    "\n",
    "raw_docs: List[Document] = []\n",
    "for p in FILE_PATHS:\n",
    "    try:\n",
    "        d = TextLoader(p, encoding=\"utf-8\").load()\n",
    "        # Attach a short source name for cleaner UI\n",
    "        for doc in d:\n",
    "            doc.metadata[\"source\"] = os.path.basename(p)\n",
    "        raw_docs.extend(d)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {p}: {e}\")\n",
    "\n",
    "# 3) Chunking (generic, keeps sections and bullets together)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,       # 600–1000 works well\n",
    "    chunk_overlap=120,    # some overlap to keep headings + bullets together\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "split_docs: List[Document] = text_splitter.split_documents(raw_docs)\n",
    "\n",
    "# Keep a global copy if your other code needs it\n",
    "GLOBAL_DOCS = split_docs\n",
    "\n",
    "# 4) Embeddings + FAISS (MMR retriever)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "mmr_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",            # Maximal Marginal Relevance to reduce redundancy\n",
    "    search_kwargs={\n",
    "        \"k\": 12,                  # retrieve more, we'll re-rank down\n",
    "        \"fetch_k\": 50,            # candidates pool size\n",
    "        \"lambda_mult\": 0.4        # trade-off diversity vs similarity (0..1)\n",
    "    }\n",
    ")\n",
    "\n",
    "# 5) Sparse BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
    "bm25_retriever.k = 20             # cast a wider net; will re-rank later\n",
    "\n",
    "# 6) Ensemble (dense + sparse) via Reciprocal Rank Fusion under the hood\n",
    "ensemble = EnsembleRetriever(\n",
    "    retrievers=[mmr_retriever, bm25_retriever],\n",
    "    weights=[0.5, 0.5]            # balanced; adjust if you prefer denser or sparser bias\n",
    ")\n",
    "\n",
    "# 7) Optional cross-encoder re-ranking (query, docs) -> top_k\n",
    "def rerank_with_cross_encoder(query: str, docs: List[Document], top_k: int = 6) -> List[Document]:\n",
    "    if _CROSS_ENCODER is None or not docs:\n",
    "        return docs[:top_k]\n",
    "    pairs = [(query, d.page_content) for d in docs]\n",
    "    scores = _CROSS_ENCODER.predict(pairs)\n",
    "    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [d for d, _ in ranked[:top_k]]\n",
    "\n",
    "# 8) A retrieve() helper compatible with your UI (returns (Document, pseudo_distance))\n",
    "def retrieve(query: str, k: int = 6) -> List[Tuple[Document, float]]:\n",
    "    # 8.1 get a fused list\n",
    "    candidates = ensemble.get_relevant_documents(query)\n",
    "    # 8.2 cross-encoder re-rank (optional) and truncate to final k\n",
    "    final_docs = rerank_with_cross_encoder(query, candidates, top_k=k)\n",
    "    if not final_docs:\n",
    "        return []\n",
    "    # 8.3 produce a pseudo-distance for your UI (invert normalized ranks)\n",
    "    # higher rank -> lower distance\n",
    "    n = len(final_docs)\n",
    "    results = []\n",
    "    for i, d in enumerate(final_docs):\n",
    "        norm_rank = i / max(1, n - 1) if n > 1 else 0.0\n",
    "        pseudo_distance = 1.0 - (1.0 - norm_rank)  # equals norm_rank\n",
    "        results.append((d, float(pseudo_distance)))\n",
    "    return results\n",
    "\n",
    "# 9) Utility for your Gradio UI (snippets display)\n",
    "def build_context_snippets(retrieved: List[Tuple[Document, float]]) -> List[str]:\n",
    "    snippets = []\n",
    "    for (doc, dist) in retrieved:\n",
    "        src = doc.metadata.get(\"source\", \"unknown.txt\")\n",
    "        preview = doc.page_content.strip().replace(\"\\n\", \" \")\n",
    "        if len(preview) > 280:\n",
    "            preview = preview[:280].rstrip() + \"...\"\n",
    "        snippets.append(f\"[{src}] (score={dist:.4f}): {preview}\")\n",
    "    return snippets\n",
    "\n",
    "# 10) Strict grounded answer chain with generic fallback (no hand-crafted logic)\n",
    "# NOTE: This assumes you already initialized `llm` elsewhere (GPT4All or Flan-T5).\n",
    "STRICT_RAG_PROMPT = \"\"\"You are a grounded assistant. Use ONLY the context to answer.\n",
    "- If the answer is in the context, answer concisely.\n",
    "- If not present, say: \"Not found in the uploaded files.\"\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "GK_FALLBACK_PROMPT = \"\"\"Answer the question using general knowledge. Be factual and concise.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "def rag_answer(question: str, k: int = 6):\n",
    "    # 10.1 Retrieve\n",
    "    hits = retrieve(question, k=k)\n",
    "\n",
    "    # 10.2 Build context and snippets\n",
    "    snippets = build_context_snippets(hits)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc, _ in hits]) if hits else \"\"\n",
    "\n",
    "    # 10.3 If no LLM is configured, just return context\n",
    "    if 'llm' not in globals() or llm is None:\n",
    "        if context:\n",
    "            return \"RAG (GPT4All)\", snippets, \"LLM not configured. Retrieved context shown above.\"\n",
    "        else:\n",
    "            return \"RAG (GPT4All)\", [], \"LLM not configured and no context retrieved.\"\n",
    "\n",
    "    # 10.4 Ask grounded first\n",
    "    grounded = llm.invoke(STRICT_RAG_PROMPT.format(question=question, context=context))\n",
    "    grounded_clean = (grounded or \"\").strip()\n",
    "\n",
    "    # If the grounded answer claims no info or is empty → GK fallback (your requested behavior)\n",
    "    if (not grounded_clean) or (\"not found in the uploaded files\" in grounded_clean.lower()):\n",
    "        gk = llm.invoke(GK_FALLBACK_PROMPT.format(question=question))\n",
    "        gk_clean = (gk or \"\").strip()\n",
    "        # Clearly mark as fallback\n",
    "        final_answer = gk_clean if gk_clean else \"No answer produced.\"\n",
    "        branch = \"RAG (GPT4All) + General Knowledge Fallback\"\n",
    "    else:\n",
    "        final_answer = grounded_clean\n",
    "        branch = \"RAG (GPT4All)\"\n",
    "\n",
    "    return branch, snippets, final_answer\n",
    "\n",
    "# ============================\n",
    "# Sanity tests you can try (outside of Gradio)\n",
    "# ============================\n",
    "# print(rag_answer(\"Explain the negative feedback of Amazon Echo Dot\")[2])\n",
    "# print(rag_answer(\"What are the colors available for Echo Dot?\")[2])\n",
    "# print(rag_answer(\"What is Amazon’s return policy?\")[2])\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 6) Dictionary Agent — Free Dictionary API\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "FREE_DICT_BASE = \"https://api.dictionaryapi.dev/api/v2/entries/en/\"\n",
    "\n",
    "def dictionary_lookup_single(term: str) -> str:\n",
    "    \"\"\"\n",
    "    Lookup one word/phrase via Free Dictionary API (dictionaryapi.dev).\n",
    "    Returns a concise definition string or a fallback message.\n",
    "    \"\"\"\n",
    "    clean = term.strip()\n",
    "    if not clean:\n",
    "        return \"Empty term.\"\n",
    "\n",
    "    url = FREE_DICT_BASE + urllib.parse.quote(clean)\n",
    "    try:\n",
    "        r = requests.get(url, timeout=8)\n",
    "        if r.status_code != 200:\n",
    "            return f\"No entry found for '{term}'.\"\n",
    "\n",
    "        data = r.json()\n",
    "        # Expected shape: list[ entry ], pick first entry’s first available definition(s)\n",
    "        defs = []\n",
    "        if isinstance(data, list) and data:\n",
    "            entry = data[0]\n",
    "            for m in entry.get(\"meanings\", []):\n",
    "                for d in m.get(\"definitions\", []):\n",
    "                    text = (d.get(\"definition\") or \"\").strip()\n",
    "                    if text:\n",
    "                        defs.append(text)\n",
    "                if defs:\n",
    "                    break  # keep it concise: stop after first meaning that has defs\n",
    "        if not defs:\n",
    "            return f\"No entry found for '{term}'.\"\n",
    "\n",
    "        # Keep it concise (up to 2 short definitions)\n",
    "        preview = \" | \".join(defs[:2])\n",
    "        return f\"{term}: {preview}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching definition for '{term}': {e}\"\n",
    "\n",
    "def dictionary_agent(query: str):\n",
    "    \"\"\"\n",
    "    Supports:\n",
    "      - 'define echo dot'             -> single phrase 'echo dot'\n",
    "      - 'define echo, dot, policy'    -> multiple comma-separated terms\n",
    "      - 'meaning of echo dot'         -> single phrase\n",
    "    \"\"\"\n",
    "    q = query.strip().lower()\n",
    "\n",
    "    # Extract the raw term(s) after 'define' or 'meaning of'\n",
    "    terms_raw = q\n",
    "    if q.startswith(\"define \"):\n",
    "        terms_raw = q[len(\"define \"):].strip()\n",
    "    elif \"meaning of \" in q:\n",
    "        terms_raw = q.split(\"meaning of \", 1)[1].strip()\n",
    "    elif q.startswith(\"define:\"):\n",
    "        terms_raw = q[len(\"define:\"):].strip()\n",
    "\n",
    "    # If commas present → multiple words; otherwise treat the entire remainder as ONE phrase\n",
    "    if \",\" in terms_raw:\n",
    "        terms = [t.strip() for t in terms_raw.split(\",\") if t.strip()]\n",
    "    else:\n",
    "        terms = [terms_raw.strip()] if terms_raw else []\n",
    "\n",
    "    if not terms:\n",
    "        return \"Dictionary Agent\", [\"No document context required\"], \"Please provide a word or phrase to define.\"\n",
    "\n",
    "    results = [dictionary_lookup_single(t) for t in terms]\n",
    "\n",
    "    branch = \"Dictionary Agent\"\n",
    "    snippets = [\"Definitions fetched via Free Dictionary API (dictionaryapi.dev)\"]\n",
    "    final_answer = \"\\n\".join(results)\n",
    "    return branch, snippets, final_answer\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    " #Calculator Agent\n",
    "\n",
    "import re\n",
    "import numexpr as ne\n",
    "\n",
    "MONTHS = [\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\n",
    "          \"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]\n",
    "STOPWORDS = {\n",
    "    \"calculate\",\"total\",\"combined\",\"units\",\"unit\",\"sold\",\"sales\",\"sale\",\"of\",\"the\",\"a\",\"an\",\"for\",\"to\",\"in\",\"on\",\"by\",\"with\",\n",
    "    \"percentage\",\"percent\",\"increase\",\"decrease\",\"growth\",\"from\",\"between\",\"and\",\"till\",\"until\",\"upto\",\"up to\",\"average\",\"mean\",\n",
    "    \"highest\",\"lowest\",\"max\",\"min\",\"which\",\"month\",\"has\",\"have\",\"does\",\"is\"\n",
    "}\n",
    "\n",
    "# Accept digits, decimal point, arithmetic ops, parentheses, and spaces\n",
    "# We'll normalize × x ÷ – — to * / - respectively before matching.\n",
    "ARITH_ALLOWED = re.compile(r\"^[0-9\\.\\+\\-\\*/\\(\\)\\s]+$\")\n",
    "\n",
    "# Find candidate arithmetic spans inside a longer query (choose the longest)\n",
    "ARITH_SPAN = re.compile(r\"(?:\\(?\\s*[-+]?\\d+(?:\\.\\d+)?\\s*\\)?\\s*(?:[\\+\\-\\*/]\\s*\\(?\\s*[-+]?\\d+(?:\\.\\d+)?\\s*\\)?\\s*)+)\")\n",
    "\n",
    "\n",
    "def _normalize_expr(expr: str) -> str:\n",
    "    # normalize common symbols to standard ops\n",
    "    expr = expr.replace(\"×\", \"*\").replace(\"x\", \"*\").replace(\"X\", \"*\").replace(\"·\", \"*\")\n",
    "    expr = expr.replace(\"÷\", \"/\")\n",
    "    expr = expr.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "    # remove thousands separators\n",
    "    expr = expr.replace(\",\", \" \")\n",
    "    # collapse multiple spaces\n",
    "    expr = re.sub(r\"\\s+\", \" \", expr).strip()\n",
    "    return expr\n",
    "\n",
    "\n",
    "def _is_safe_expr(expr: str) -> bool:\n",
    "    return bool(ARITH_ALLOWED.match(expr))\n",
    "\n",
    "\n",
    "def _extract_inline_expr(query: str):\n",
    "    q = _normalize_expr(query)\n",
    "    # collect all arithmetic-looking spans and choose the longest (most likely full expression)\n",
    "    spans = ARITH_SPAN.findall(q)\n",
    "    if not spans:\n",
    "        return None\n",
    "    # choose the longest by length\n",
    "    expr = max((s.strip() for s in spans), key=len)\n",
    "    expr = _normalize_expr(expr)\n",
    "    if not _is_safe_expr(expr):\n",
    "        return None\n",
    "    # Evaluate with BODMAS precedence via numexpr\n",
    "    try:\n",
    "        val = ne.evaluate(expr).item()\n",
    "        return expr, val\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _product_phrases_and_tokens(query: str):\n",
    "    q = query.lower()\n",
    "    # split on commas and \" and \"\n",
    "    chunks = re.split(r\"\\s*,\\s*|\\s+and\\s+\", q)\n",
    "    phrases = []\n",
    "    for ch in chunks:\n",
    "        words = [w for w in re.findall(r\"[a-zA-Z]+\", ch) if w not in STOPWORDS]\n",
    "        if words:\n",
    "            phrases.append(\" \".join(words))\n",
    "    flat_tokens = [w for w in re.findall(r\"[a-zA-Z]+\", q) if w not in STOPWORDS]\n",
    "    return [p for p in phrases if p], flat_tokens\n",
    "\n",
    "\n",
    "def _split_product_blocks(text: str):\n",
    "    # each block starts with \"Product:\"\n",
    "    return [b.strip() for b in re.split(r\"(?=^Product:\\s*)\", text, flags=re.IGNORECASE | re.MULTILINE) if b.strip()]\n",
    "\n",
    "\n",
    "def _block_matches(block: str, phrase: str, flat_tokens: list):\n",
    "    b = block.lower()\n",
    "    if phrase and phrase in b:\n",
    "        return True\n",
    "    return all(t in b for t in flat_tokens) if flat_tokens else False\n",
    "\n",
    "\n",
    "def _month_values_from_block(block: str):\n",
    "    \"\"\"\n",
    "    Parse lines like \"January: 150 units sold\" -> {\"january\": 150, ...}\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for line in block.splitlines():\n",
    "        m = re.search(r\"([A-Za-z]+):\\s*(\\d+)\\s+units\\s+sold\", line)\n",
    "        if m:\n",
    "            mon = m.group(1).lower()\n",
    "            val = int(m.group(2))\n",
    "            if mon in MONTHS:\n",
    "                data[mon] = val\n",
    "    return data\n",
    "\n",
    "\n",
    "def _pick_two_months_from_query(q_lower: str):\n",
    "    found = [m for m in MONTHS if m in q_lower]\n",
    "    if len(found) >= 2:\n",
    "        return found[0], found[-1]    # first and last mention\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def calculator_agent(query: str, k: int = 5, distance_threshold: float = 1.5):\n",
    "    try:\n",
    "        # ---------- NEW: Direct arithmetic (BODMAS) path FIRST ----------\n",
    "        inline = _extract_inline_expr(query)\n",
    "        if inline:\n",
    "            expr, val = inline\n",
    "            # No need for doc context; we evaluated a self-contained expression\n",
    "            return \"Calculator Agent\", [\"Direct expression (BODMAS) — no document context required\"], f\"Expression: {expr}\\nResult: {val}\"\n",
    "\n",
    "        # ---------- Existing doc-aware logic below ----------\n",
    "        retrieved = retrieve(query, k=k)\n",
    "        relevant = [r for r in retrieved if r[1] <= distance_threshold]\n",
    "        snippets = build_context_snippets(retrieved if relevant else [])\n",
    "\n",
    "        # Build context blob and split into product blocks\n",
    "        context = \"\\n\\n\".join([r[0].page_content for r in relevant]) if relevant else \"\"\n",
    "        blocks = _split_product_blocks(context) if context else []\n",
    "\n",
    "        # Identify product blocks from query (multi-word supported)\n",
    "        phrases, flat_tokens = _product_phrases_and_tokens(query)\n",
    "        matched_blocks = []\n",
    "        for ph in phrases:\n",
    "            matched_blocks += [b for b in blocks if _block_matches(b, ph, [])]\n",
    "        if not matched_blocks and flat_tokens:\n",
    "            matched_blocks = [b for b in blocks if _block_matches(b, \"\", flat_tokens)]\n",
    "        if not matched_blocks and blocks:\n",
    "            # last resort: pick most relevant block (first)\n",
    "            matched_blocks = [blocks[0]]\n",
    "\n",
    "        ql = query.lower()\n",
    "\n",
    "        # ---------------- Deterministic extrema answers (NO LLM) ----------------\n",
    "        if any(w in ql for w in [\"highest\",\"max\",\"peak\"]) and matched_blocks:\n",
    "            data = _month_values_from_block(matched_blocks[0])\n",
    "            if data:\n",
    "                best_month = max(data, key=data.get)\n",
    "                return \"Calculator Agent\", [matched_blocks[0]], f\"Result: {best_month.title()} with {data[best_month]} units\"\n",
    "\n",
    "        if any(w in ql for w in [\"lowest\",\"min\"]) and matched_blocks:\n",
    "            data = _month_values_from_block(matched_blocks[0])\n",
    "            if data:\n",
    "                worst_month = min(data, key=data.get)\n",
    "                return \"Calculator Agent\", [matched_blocks[0]], f\"Result: {worst_month.title()} with {data[worst_month]} units\"\n",
    "\n",
    "        # ---------------- Percentage increase/decrease ----------------\n",
    "        if any(w in ql for w in [\"percentage\",\"percent\",\"increase\",\"decrease\",\"growth\"]) and matched_blocks:\n",
    "            data = _month_values_from_block(matched_blocks[0])\n",
    "            m1, m2 = _pick_two_months_from_query(ql)\n",
    "            if m1 and m2 and m1 in data and m2 in data:\n",
    "                v1, v2 = data[m1], data[m2]\n",
    "                expr = f\"({v2} - {v1}) / {v1} * 100\"\n",
    "                result = (v2 - v1) / v1 * 100 if v1 != 0 else float('nan')\n",
    "                return \"Calculator Agent\", [matched_blocks[0]], f\"Expression: {expr}\\nResult: {result:.2f}%\"\n",
    "\n",
    "        # ---------------- Totals / combined totals ----------------\n",
    "        if any(w in ql for w in [\"total\",\"sum\",\"combined\"]) and matched_blocks:\n",
    "            expr_parts, totals, used = [], [], []\n",
    "            for b in matched_blocks:\n",
    "                data = _month_values_from_block(b)\n",
    "                nums = list(data.values())\n",
    "                if nums:\n",
    "                    expr_parts.append(\" + \".join(map(str, nums)))\n",
    "                    totals.append(sum(nums))\n",
    "                    used.append(b)\n",
    "            if totals:\n",
    "                if len(totals) > 1:\n",
    "                    expr = \" + \".join(f\"({p})\" for p in expr_parts)\n",
    "                    res = sum(totals)\n",
    "                else:\n",
    "                    expr = expr_parts[0]\n",
    "                    res = totals[0]\n",
    "                return \"Calculator Agent\", used, f\"Expression: {expr}\\nResult: {res}\"\n",
    "\n",
    "        # ---------------- Average (mean) ----------------\n",
    "        if any(w in ql for w in [\"average\",\"mean\",\"avg\"]) and matched_blocks:\n",
    "            data = _month_values_from_block(matched_blocks[0])\n",
    "            nums = list(data.values())\n",
    "            if nums:\n",
    "                expr = f\"({'+'.join(map(str, nums))}) / {len(nums)}\"\n",
    "                res = sum(nums)/len(nums)\n",
    "                return \"Calculator Agent\", [matched_blocks[0]], f\"Expression: {expr}\\nResult: {res:.2f}\"\n",
    "\n",
    "        # ---------------- Difference (absolute) if two months given ----------------\n",
    "        if any(w in ql for w in [\"difference\",\"more\",\"less\"]) and matched_blocks:\n",
    "            data = _month_values_from_block(matched_blocks[0])\n",
    "            m1, m2 = _pick_two_months_from_query(ql)\n",
    "            if m1 and m2 and m1 in data and m2 in data:\n",
    "                v1, v2 = data[m1], data[m2]\n",
    "                expr = f\"{v2} - {v1}\"\n",
    "                res  = v2 - v1\n",
    "                return \"Calculator Agent\", [matched_blocks[0]], f\"Expression: {expr}\\nResult: {res}\"\n",
    "\n",
    "        # ---------------- Ratios between two products ----------------\n",
    "        if any(w in ql for w in [\"ratio\",\"times\",\"compared\",\"compare\"]) and len(matched_blocks) >= 2:\n",
    "            d1 = _month_values_from_block(matched_blocks[0])\n",
    "            d2 = _month_values_from_block(matched_blocks[1])\n",
    "            if d1 and d2:\n",
    "                s1, s2 = sum(d1.values()), sum(d2.values())\n",
    "                expr = f\"{s1} / {s2}\"\n",
    "                res  = s1 / s2 if s2 != 0 else float('inf')\n",
    "                return \"Calculator Agent\", matched_blocks[:2], f\"Expression: {expr}\\nResult: {res:.2f}\"\n",
    "\n",
    "        # ---------------- Default deterministic sum of first block (as a safe fallback) ----------------\n",
    "        if matched_blocks:\n",
    "            data = _month_values_from_block(matched_blocks[0])\n",
    "            nums = list(data.values())\n",
    "            if nums:\n",
    "                expr = \" + \".join(map(str, nums))\n",
    "                res  = sum(nums)\n",
    "                return \"Calculator Agent\", [matched_blocks[0]], f\"Expression: {expr}\\nResult: {res}\"\n",
    "\n",
    "        # ---------------- Safe LLM fallback (only if llm & prompt exist) ----------------\n",
    "        if relevant and 'llm' in globals() and llm:\n",
    "            local_prompt = (\n",
    "                MATH_PROMPT if 'MATH_PROMPT' in globals() and MATH_PROMPT\n",
    "                else \"Produce ONE arithmetic expression (numbers and + - * / only) that answers the question.\\nNo words.\\nQuestion: {question}\\nExpression:\"\n",
    "            )\n",
    "            ctx = \"\\n\\n\".join([r[0].page_content for r in relevant])\n",
    "            raw = llm.invoke(local_prompt.format(question=query, context=ctx))\n",
    "            m = re.search(r\"([0-9\\.\\(\\)\\+\\-\\*/\\s]+)\", (raw or \"\"))\n",
    "            expr = (m.group(1) if m else \"\").replace(\",\", \"\").strip()\n",
    "            if expr:\n",
    "                try:\n",
    "                    result = ne.evaluate(expr).item()\n",
    "                    return \"Calculator Agent\", snippets if snippets else [\"No document context found\"], f\"Expression: {expr}\\nResult: {result}\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # ---------------- Final fallback (no crash) ----------------\n",
    "        return \"Calculator Agent\", snippets if snippets else [\"No document context found\"], \\\n",
    "               \"Could not compute a result from the available information.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Never let exceptions bubble to Gradio as 'Error'\n",
    "        return \"Calculator Agent\", [\"Error while computing\"], f\"Error: {e}\"\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ========= Drop-in patch: stop echo + ensure grounded answer =========\n",
    "\n",
    "# 1) Prompts\n",
    "RAG_TEMPLATE = \"\"\"### Instruction:\n",
    "Answer ONLY using the provided context. If the answer is not in the context, reply exactly:\n",
    "Not found in the uploaded files.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "RAG_TEMPLATE_COMPACT = (\n",
    "    # Minimal template for retry (works better for small models)\n",
    "    \"Use ONLY the context to answer. If not in context, reply exactly: Not found in the uploaded files.\\n\\n\"\n",
    "    \"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "GK_FALLBACK_TEMPLATE = (\n",
    "    \"Answer the question using general knowledge. Be factual and concise.\\n\\n\"\n",
    "    \"Question:\\n{question}\\n\\nAnswer:\"\n",
    ")\n",
    "\n",
    "# 2) Stop tokens (prevents the model from drifting back into headers)\n",
    "STOP_TOKENS = [\"\\n###\", \"### Question:\", \"### Context:\", \"### Instruction:\", \"\\nQuestion:\", \"\\nContext:\"]\n",
    "\n",
    "# 3) Safer LLM call that prefers `.generate(..., stop=...)`, else falls back to `.invoke`\n",
    "def _llm_call(prompt: str, stops: list[str] = None, max_retries: int = 1) -> str:\n",
    "    if 'llm' not in globals() or llm is None:\n",
    "        return \"\"\n",
    "    stops = stops or STOP_TOKENS\n",
    "\n",
    "    # Prefer generate to pass stop tokens\n",
    "    if hasattr(llm, \"generate\"):\n",
    "        try:\n",
    "            out = llm.generate([prompt], stop=stops)  # stop works for many LangChain LLMs\n",
    "            txt = out.generations[0][0].text if out and out.generations and out.generations[0] else \"\"\n",
    "            if txt:\n",
    "                return txt.strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Fallback to invoke (no stop support in many wrappers)\n",
    "    txt = \"\"\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            txt = llm.invoke(prompt)\n",
    "            if txt:\n",
    "                return txt.strip()\n",
    "        except Exception:\n",
    "            continue\n",
    "    return (txt or \"\").strip()\n",
    "\n",
    "# 4) Build concise context to reduce prompt echo\n",
    "def _build_context_text(hits, max_chars: int = 2000) -> str:\n",
    "    if not hits:\n",
    "        return \"\"\n",
    "    parts, total = [], 0\n",
    "    for doc, _ in hits:\n",
    "        chunk = (doc.page_content or \"\").strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        add = chunk[: max(0, max_chars - total)]\n",
    "        if not add:\n",
    "            break\n",
    "        parts.append(add)\n",
    "        total += len(add)\n",
    "        if total >= max_chars:\n",
    "            break\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "def _looks_like_echo(txt: str) -> bool:\n",
    "    if not txt:\n",
    "        return True\n",
    "    tl = txt.lower()\n",
    "    return (\n",
    "        \"### instruction\" in tl\n",
    "        or \"### context\" in tl\n",
    "        or \"### question\" in tl\n",
    "        or \"answer only using the provided context\" in tl\n",
    "        or len(tl.strip()) <= 10\n",
    "    )\n",
    "\n",
    "def rag_answer(question: str, k: int = 6):\n",
    "    # Retrieve as usual\n",
    "    hits = retrieve(question, k=k)\n",
    "    snippets = build_context_snippets(hits)\n",
    "    context = _build_context_text(hits, max_chars=2000)\n",
    "\n",
    "    # If no context at all → GK fallback immediately\n",
    "    if not context:\n",
    "        gk = _llm_call(GK_FALLBACK_TEMPLATE.format(question=question), STOP_TOKENS)\n",
    "        return \"RAG (FLAN-T5)\", snippets, (gk or \"No answer produced.\")  # or keep your old label\n",
    "\n",
    "    # First attempt: full grounded prompt + stop tokens\n",
    "    grounded_1 = _llm_call(RAG_TEMPLATE.format(question=question, context=context), STOP_TOKENS)\n",
    "    grounded_1 = (grounded_1 or \"\").strip()\n",
    "    if grounded_1 and (not _looks_like_echo(grounded_1)) and (\"not found in the uploaded files\" not in grounded_1.lower()):\n",
    "        return \"RAG (FLAN-T5)\", snippets, grounded_1  # label text is up to you\n",
    "\n",
    "    # Retry once with a compact prompt and only the top-1 chunk (strongly reduces echo)\n",
    "    top1_ctx = _build_context_text(hits[:1], max_chars=1000)\n",
    "    grounded_2 = _llm_call(RAG_TEMPLATE_COMPACT.format(question=question, context=top1_ctx), STOP_TOKENS)\n",
    "    grounded_2 = (grounded_2 or \"\").strip()\n",
    "    if grounded_2 and (not _looks_like_echo(grounded_2)) and (\"not found in the uploaded files\" not in grounded_2.lower()):\n",
    "        return \"RAG (FLAN-T5)\", snippets, grounded_2\n",
    "\n",
    "    # Final fallback: GK (you asked to allow general knowledge if not found)\n",
    "    gk = _llm_call(GK_FALLBACK_TEMPLATE.format(question=question), STOP_TOKENS)\n",
    "    gk = (gk or \"\").strip() or \"No answer produced.\"\n",
    "    return \"RAG (FLAN-T5) + General Knowledge Fallback\", snippets, gk\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Smarter Router: numeric vs. reviews/sentiment vs. dictionary vs. RAG ---\n",
    "import re\n",
    "\n",
    "# Strong calc cues (not generic words like \"sales\")\n",
    "CALC_STRONG = {\n",
    "    \"calculate\",\"sum\",\"total\",\"combined\",\"add\",\"plus\",\n",
    "    \"average\",\"avg\",\"mean\",\"median\",\n",
    "    \"difference\",\"delta\",\"gap\",\"increase\",\"decrease\",\"growth\",\"change\",\n",
    "    \"percent\",\"percentage\",\"%\",\"ratio\",\"times\",\"compared\",\"compare\",\n",
    "    \"highest\",\"lowest\",\"max\",\"min\",\"peak\",\n",
    "    \"ytd\",\"cumulative\"\n",
    "}\n",
    "MONTHS = {\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\n",
    "          \"august\",\"september\",\"october\",\"november\",\"december\"}\n",
    "\n",
    "# Reviews / sentiment / pros & cons → RAG\n",
    "REVIEW_KEYWORDS = {\n",
    "    \"review\",\"reviews\",\"feedback\",\"negative\",\"positive\",\"complaint\",\"complaints\",\n",
    "    \"issue\",\"issues\",\"pros\",\"cons\",\"drawbacks\",\"limitations\",\"problems\",\"impression\",\"sentiment\"\n",
    "}\n",
    "\n",
    "INLINE_MATH_RE = re.compile(r\"(?:\\d+[\\d\\.,]*|\\.\\d+)(?:\\s*[\\+\\-\\*/]\\s*(?:\\d+[\\d\\.,]*|\\.\\d+))+\")\n",
    "ANY_DIGIT_RE   = re.compile(r\"\\d\")\n",
    "\n",
    "def _has_inline_math(q: str) -> bool:\n",
    "    return INLINE_MATH_RE.search(q.replace(\",\", \"\")) is not None\n",
    "\n",
    "def _looks_numeric(q: str) -> bool:\n",
    "    ql = q.lower()\n",
    "    # must have: inline math OR a strong calc cue OR month name OR an actual digit\n",
    "    return (\n",
    "        _has_inline_math(ql)\n",
    "        or any(k in ql for k in CALC_STRONG)\n",
    "        or any(m in ql for m in MONTHS)\n",
    "        or bool(ANY_DIGIT_RE.search(ql))\n",
    "    )\n",
    "\n",
    "def _looks_review(q: str) -> bool:\n",
    "    ql = q.lower()\n",
    "    return any(k in ql for k in REVIEW_KEYWORDS)\n",
    "\n",
    "def choose_branch(query: str) -> str:\n",
    "    q = query.strip().lower()\n",
    "\n",
    "    # Dictionary agent\n",
    "    if q.startswith(\"define \") or \"meaning of \" in q or \"define:\" in q:\n",
    "        return \"dictionary\"\n",
    "\n",
    "    # Reviews / sentiment → RAG\n",
    "    if _looks_review(q):\n",
    "        return \"rag\"\n",
    "\n",
    "    # Numeric reasoning → Calculator\n",
    "    if _looks_numeric(q):\n",
    "        return \"calculator\"\n",
    "\n",
    "    # Default → RAG\n",
    "    return \"rag\"\n",
    "\n",
    "# --- Main dispatcher with second-chance override for numeric queries ---\n",
    "def answer_query(query: str):\n",
    "    initial = choose_branch(query)\n",
    "\n",
    "    try:\n",
    "        if initial == \"calculator\":\n",
    "            b, snippets, final = calculator_agent(query)\n",
    "            return b, \"\\n\\n\".join(snippets), final\n",
    "\n",
    "        if initial == \"dictionary\":\n",
    "            b, snippets, final = dictionary_agent(query)\n",
    "            return b, \"\\n\\n\".join(snippets), final\n",
    "\n",
    "        # initial == \"rag\"\n",
    "        # If the query also looks numeric, prefer Calculator (safety net)\n",
    "        if _looks_numeric(query):\n",
    "            b, snippets, final = calculator_agent(query)\n",
    "            return b, \"\\n\\n\".join(snippets), final\n",
    "\n",
    "        b, snippets, final = rag_answer(query)\n",
    "        return b, \"\\n\\n\".join(snippets), final\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"Error\", f\"(debug) {type(e).__name__}: {e}\", \"An error occurred while processing your query.\"\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 9) Gradio UI with timeout\n",
    "import gradio as gr\n",
    "import concurrent.futures\n",
    "\n",
    "TIMEOUT_SECS = 150  # 2 minutes\n",
    "\n",
    "def gradio_fn(user_query):\n",
    "    try:\n",
    "        # Run answer_query with timeout\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            future = executor.submit(answer_query, user_query)\n",
    "            try:\n",
    "                branch, snippets, final = future.result(timeout=TIMEOUT_SECS)\n",
    "                return branch, snippets, final\n",
    "            except concurrent.futures.TimeoutError:\n",
    "                return \"Error\", \"Timed out\", \"Answer not found (took more than 2 minutes).\"\n",
    "    except Exception as e:\n",
    "        return \"Error\", f\"(debug) {type(e).__name__}: {e}\", \"An error occurred while processing your query.\"\n",
    "\n",
    "with gr.Blocks(title=\"RAG-Powered Multi-Agent Q&A\") as demo:\n",
    "    gr.Markdown(\"## RAG-Powered Multi-Agent Q&A (LangChain)\")\n",
    "\n",
    "    inp = gr.Textbox(label=\"Ask a question…\", placeholder=\"Type your query here...\")\n",
    "    with gr.Row():\n",
    "        out_branch = gr.Textbox(label=\"Which tool/agent branch was used\")\n",
    "    out_snippets = gr.Textbox(label=\"The retrieved context snippets\", lines=10)\n",
    "    out_final = gr.Textbox(label=\"The final answer\", lines=8)\n",
    "\n",
    "    btn = gr.Button(\"Run\")\n",
    "    btn.click(gradio_fn, inputs=inp, outputs=[out_branch, out_snippets, out_final])\n",
    "\n",
    "demo.launch(share=True)\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}